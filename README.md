# simple-cpu-dra-driver

## Overview

The `simple-cpu-dra-driver` is a Kubernetes
[Dynamic Resource Allocation (DRA)](https://kubernetes.io/docs/concepts/scheduling-eviction/dynamic-resource-allocation/)
driver that enables CPU pinning for workloads using DRA framework. This project
uses a combination of a DRA driver and an NRI (Node Resource Interface) plugin
to manage CPU allocations. It ensures that high-priority pods requesting
exclusive CPUs through Resource Claim get them, while also managing the
remaining CPUs for shared (BestEffort) pods.

This implements the proposal from the
[feasibility doc](https://docs.google.com/document/d/1Tb_dC60YVCBr7cNYWuVLddUUTMcNoIt3zjd5-8rgug0/edit?tab=t.0#heading=h.iutbebngx80e)

## Requirements

- Kubernetes Version: v1.33 or higher.
- Feature Gates: The following feature gates must be enabled in your cluster.
  - `DynamicResourceAllocation`
  - `DRAResourceClaimDeviceStatus`
- Kubelet CPUManager: The Kubelet's default CPUManager policy must be set to
  none, as this driver takes full responsibility for CPU pinning.
- Pod Specs: Pods requesting exclusive CPUs via a ResourceClaim should also
  include standard CPU requests to ensure the Kubernetes scheduler can make
  correct placement decisions.

## How it Works

The driver is deployed as a DaemonSet which contains two core components:

- **DRA driver**: This component is responsible for discovering the CPU topology
  of the node and reporting the available CPUs as allocatable resources to the
  Kubernetes scheduler by creating `ResourceSlice` objects. When a resource
  claim is allocated, the driver generates a CDI (Container Device Interface)
  specification that tells the container runtime to inject an environment
  variable with the assigned CPU set into the container.

- **NRI Plugin**: This component integrates with the container runtime via the
  Node Resource Interface (NRI).

  - For containers with **guaranteed CPUs**, the plugin reads the environment
    variable injected via CDI and pins the container to its exclusive CPU set.
  - For all other containers, it confines them to a **shared pool** of CPUs that
    are not exclusively allocated.
  - It dynamically updates the shared pool as guaranteed containers are created
    or removed, ensuring efficient use of resources.

## Feature Support

### Currently Supported

- Exclusive CPU Allocation: Guaranteed pods that request CPUs via a
  ResourceClaim are allocated exclusive cores.
- Shared CPU Pool Management: All other containers without a ResourceClaim are
  confined to a shared pool of CPUs that are not reserved for Guaranteed pods.
- Handle daemonset restart. On restart, the driver synchronizes with all
  existing pods on the node to rebuild its state of CPU allocations, ensuring
  accurate CPU allocation for newly scheduled pods.

### Not Supported

- This driver currently only manages CPU resources. Memory allocation and
  management are not supported.

## Getting Started

### Installation

- Create a kind cluster
  - `kind create cluster --config kind.yaml`
- Deploy the driver and all necessary RBAC configurations using the provided
  manifest
  - `kubectl apply -f install.yaml`

### Example Usage

- Create a DeviceClass: This tells Kubernetes how to find the resources provided
  by this driver.
  - `kubectl apply -f examples/sample_device_class.yaml`
- Create a ResourceClaim: This requests a specific number of exclusive CPUs from
  the driver.
  - `kubectl apply -f examples/sample_cpu_resource_claim.yaml`
- Create a Pod: Reference the ResourceClaim in your pod spec to receive the
  allocated CPUs.
  - `kubectl apply -f examples/sample_pod_with_resource_claim.yaml`

### Example Driver Artifacts

- ResourceSLice (generated by the driver)

```
apiVersion: resource.k8s.io/v1beta2
kind: ResourceSlice
metadata:
  creationTimestamp: "2025-07-08T23:30:49Z"
  generateName: kind-worker-dra.cpu-
  generation: 1
  name: kind-worker-dra.cpu-qbxxr
  ownerReferences:
  - apiVersion: v1
    controller: true
    kind: Node
    name: kind-worker
    uid: cc541bab-f80a-454d-9995-8e24d6cd69c0
  resourceVersion: "746"
  uid: fab29215-023f-422f-ab7e-42c437de7d49
spec:
  devices:
  - name: cpudev0
    basic:
      attributes:
        dra.cpu/coreType: p-core
        dra.cpu/l3CacheID: 0
        dra.cpu/numaNode: 0
        dra.cpu/socketID: 0
  - name: cpudev1
    basic:
      attributes:
        dra.cpu/coreType: p-core
        dra.cpu/l3CacheID: 0
        dra.cpu/numaNode: 0
        dra.cpu/socketID: 0
  - name: cpudev2
    basic:
      attributes:
        dra.cpu/coreType: p-core
        dra.cpu/l3CacheID: 0
        dra.cpu/numaNode: 0
        dra.cpu/socketID: 0
...
...
...
  - name: cpudev119
    basic:
      attributes:
        dra.cpu/coreType: p-core
        dra.cpu/l3CacheID: 1
        dra.cpu/numaNode: 1
        dra.cpu/socketID: 1
  driver: dra.cpu
  nodeName: kind-worker
  pool:
    generation: 1
    name: kind-worker
    resourceSliceCount: 1
```

- DeviceClass, ResourceClaim objects included in `examples/` dir.

- CDI file on the node

```
    root@kind-worker:/var/run/cdi# ls
    dra.cpu.json
    root@kind-worker:/var/run/cdi# cat dra.cpu.json
    {
    "cdiVersion": "0.8.0",
    "kind": "dra.k8s.io/cpu",
    "devices": [
        {
        "name": "claim-826e3ba8-ab40-43fd-8167-5896960b591e",
        "containerEdits": {
            "env": [
            "DRA_CPUSET_claim_dummy-cpu-request=0-3"
            ]
        }
        },
        {
        "name": "claim-611ecefc-bc8e-49ca-93a5-21019ba2a660",
        "containerEdits": {
            "env": [
            "DRA_CPUSET_claim_dummy-cpu-request-2=4-9"
            ]
        }
        }
    ]
    }
```
